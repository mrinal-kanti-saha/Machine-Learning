{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Questions-I from MDSC-301(P)\n",
    "\n",
    "----------------------------------------------------------------\n",
    "Author: Mrinal Kanti Saha\n",
    "\n",
    "Date: September 5, 2022\n",
    "\n",
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. Which Linear Regression training algorithm can you use if you have\n",
    "a training set with millions of features?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent and its flavours, Mini-Batch and Stochastic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2. Suppose the features in your training set have very different scales.\n",
    "Which algorithms might suffer from this, and how? What can you\n",
    "do about it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent algorithms.\n",
    "Scale the features (Standardize or Normalise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.  Suppose you use Batch Gradient Descent and you plot the validation\n",
    "error at every epoch. If you notice that the validation error\n",
    "consistently goes up, what is likely going on? How can you fix this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is most likely overfitting the training data. We can fix this by using regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4. Is it a good idea to stop Mini-batch Gradient Descent immediately\n",
    "when the validation error goes up?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, because as it involves random batches of data, the validation loss might go up in some iteration. We have to watch out for the validation loss and if it goes up consistently for few epochs then, we would have to stop the Mini-Batch Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5. Suppose you are using Polynomial Regression. You plot the learning\n",
    "curves and you notice that there is a large gap between the training\n",
    "error and the validation error. What is happening? What are three\n",
    "ways to solve this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting.\n",
    "3 ways to solve overfitting in this case are:-\n",
    "1. Regularization\n",
    "2. Cross Validation\n",
    "3. Reducing the polynomial degree (Dimensionality Reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6. Suppose you are using Ridge Regression and you notice that the\n",
    "training error and the validation error are almost equal and fairly\n",
    "high. Would you say that the model suffers from high bias or high\n",
    "variance? Should you increase the regularization hyperparameter $\\alpha$\n",
    "or reduce it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model suffers from High bias.\n",
    "\n",
    "Reduce regularization hyperparameter $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7. Why would you want to use:**\n",
    "   - Ridge Regression instead of plain Linear Regression (i.e., without any regularization)?**\n",
    "   - Lasso instead of Ridge Regression?\n",
    "   - Elastic Net instead of Lasso?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So that our model doesn't overfit our training data, a model with regularization should be atleast as good as a plain Linear Regression model.\n",
    "- When we suspect that not all the features are to be useful for our model, Lasso is preferred as it automatically selects features by reducing the associated weights to 0, which Ridge doesn't.\n",
    "- Elastic Net is used when the number of features are greater than the number of training samples or when strongly correlated features are present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8.  Can you name four of the main challenges in Machine Learning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lack of Data.\n",
    "2. Poor Quality of Data.\n",
    "3. Overfitting and underfitting of data.\n",
    "4. Non representative Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9. If your model performs great on the training data but generalizes\n",
    "poorly to new instances, what is happening? Can you name three\n",
    "possible solutions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting.\n",
    "3 ways to solve overfitting are:-\n",
    "1. Regularization.\n",
    "2. Cross Validation.\n",
    "3. Dimensionality Reduction.\n",
    "4. Get more training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10. What is a test set, and why would you want to use it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set is used to test the performance of the model on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11.  What is the purpose of a validation set?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation set is used to check the performance of the model during training phase. Mainly, it helps in detecting whether the model is overfitting the data during training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12. What are different loss functions? Explain their importance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different loss functions are:-\n",
    "- Mean Squared Error :-\n",
    "\n",
    "    $MSE$ = $\\frac{1}{D}\\sum_{i=1}^{D}(y_i-\\hat{y_i})^2$\n",
    "\n",
    "    - Differentiable.\n",
    "    - Vulnerable to outliers.\n",
    "- Mean Absolute Error :-\n",
    "\n",
    "    $MAE$ = $\\frac{1}{D}\\sum_{i=1}^{D}|y_i-\\hat{y_i}|$\n",
    "    \n",
    "    - Considers magnitude only (not squared).\n",
    "    - Robust to outliers.\n",
    "- Hinge Loss :- \n",
    "\n",
    "    $Loss$ = $max(0, 1 - y_i \\cdot \\hat{y_i})$\n",
    "    \n",
    "    - Used for classification in SVMs.\n",
    "    - Maximizes the margin between our decision boundary and datapoints.\n",
    "- Cross Entropy Loss :-\n",
    "\n",
    "    $CELoss$ = $-\\sum_{i=1}^{C}y_{i}\\log(p_{i})$\n",
    "    \n",
    "    - Used for both binary and multi-class classification problems.\n",
    "    - Measures the difference between two probability distributions for a set of events using entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q13. Explain the following:**\n",
    "- Gradient descent\n",
    "- Mini-batch gradient descent\n",
    "- Batch gradient, and\n",
    "- Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Descent is an iterative optimization algorithm that uses gradients(slopes/derivatites) to step towards the local minima of the given function.\n",
    "\n",
    "- Mini-Batch Gradient Descent is a varient of the gradient descent algorithms, where only a fixed number of training samples are used per iteration. This is very helpful, in cases where the training data size is huge and will be computationally expensive to do Batch gradient descent. This is considered as the middle ground between Batch and Stochastic gradient descent.\n",
    "\n",
    "- Batch Gradient Descent is actually just the vanilla gradient descent, where all the training samples are used per iteration of the gradient descent. Hence, it is very computationally expensive.\n",
    "\n",
    "- Stochastic Gradient Descent is another varient of gradient descent algorithms, where onlt one randomly chosen sample is used per iteration. This can be slow if the training data is huge as the number of iteration would be huge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q14. What is learning rate?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate is a hyperparameter in a optimization algorithms that determines the step size of the algorithm at an iteration, which in turn determines how fast or slowly (or sometimes no way) the algorithm converges to the local minima. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q15. Define the following terms. Explain their importance in the data analysis.**\n",
    "- $R^2$\n",
    "- Adjusted $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2$ is a measure of how much of the variation in the Y-variable (regrssand) can be explained by the X-variables (regressor). But, the problem with $R^2$ is that addition of random variables to it never degrades the score, instead increases it no matter what. Therefore, $Adjusted R^2$ came to the rescue.\n",
    "\n",
    "$R^2 = 1 - \\frac{\\sum_{i=1}^{N}(y_i-\\hat{y_i})^2}{\\sum_{i=1}^{N}(y_i-\\bar{y})^2} = 1 - \\frac{RSS}{TSS}$\n",
    "\n",
    "$Adjusted R^2$ is a varient of the $R^2$ where every added variable (dummy) is penalised, if it doesn't add much to the performance of the model. Hence, the value of $Adjusted R^2$ will be less than or equal to $R^2$.\n",
    "\n",
    "$Adjusted R^2 = 1 - \\frac{RSS / (n-k)}{TSS / (n-1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q16. Explain One-Hot Encoding and Label Encoding.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One Hot Encoding is an encoding technique used to encode nominal categorical variables. Nominal categorical variables are those that do not follow an inherent ordering among the classes or categories of the variable. This is done by representing each category by a column of 0s and 1s to indicate the absence or presence of that category for a particular record. This is however not recommended for nominal categorical variables will huge number of classes.\n",
    "\n",
    "- Label Encoding is an encoding technique used to encode ordinal categorical variable. Ordinal categorical variables are those that do follow an inherent ordering among the classes of the variable. But, Label encoder is specifically used for the target variable only and not the independent variables, for which we have the Ordinal Encoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q17. What are the assumption on Naive Bayes algorithm in classification?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions for the Naive Bayes algorithm are:-\n",
    "- All the features are assumed to be independent.\n",
    "- The apriori probabilities for each class is either uniform or empirical.\n",
    "- The likelyhood probabilities are Normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q18. What is the difference between classification and regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression is used to predict a continuous value (like stock price) whereas Classification is used to predict discrete classes or categories (like Buy, Hold, Sell a stock)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q19. How to ensure that the model is not overfitting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that our model is not overfitting:-\n",
    "- K-fold Cross Validation - We split the dataset into K-folds (subsets) and K-1 subsets are used for training and only one subset is used for validation. This step is repeated K times, such that all of the data get used for training as well as validation purposes.\n",
    "- Early Stopping.\n",
    "- Regularization - adding a regularization term (L1 norm, L2 norm) to the loss function of the algorithm solves the issue of overfitting.\n",
    "- Get more training data.\n",
    "- Dimensionality Reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q20. List the main advantage of Naive Bayes?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages of Naive Bayes :-\n",
    "- It is a very fast algorithm and easy to implement.\n",
    "- If the 'Naive' assumption of the classier holds true, then it may perform better than any classifier and in a fraction of the time taken by other classifiers.\n",
    "- It is best for categorical predictors and not for numerical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q21. What you should do when your model is suffering from:**\n",
    "- Low bias and high variance?\n",
    "- High bias and low variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low Bias and High Variance indicate overfitting. Hence, we will have to apply given techniques to solve the problem of overfitting :\n",
    "- Regularization - adding a regularization term (L1 norm, L2 norm) to the loss function of the algorithm solves the issue of overfitting.\n",
    "- Get more training data.\n",
    "- K-fold Cross Validation - We split the dataset into K-folds (subsets) and K-1 subsets are used for training and only one subset is used for validation. This step is repeated K times, such that all of the data get used for training as well as validation purposes.\n",
    "- Early Stopping.\n",
    "- Dimensionality Reduction.\n",
    "\n",
    "High Variance and Low Bias indicate underfitting. This happens due to :-\n",
    "- Availability of very less training data to build an accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q22. What is the 'Naive' in the Naive Bayes Classifier?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is called 'Naive' as it assumes that every feature in the dataset will be independent, which is seldom true for real world data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q23. What is bias-variance tradeoff in Machine Learning ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average squared error in Regression can be decomposed into $Bias^2$, $Variance$ and Irreducible error. As Irreducible error cannot be reduced further, to reduce error further we can either reduce bias or variance. If bias has to be reduced, then the model complexity has to be increased, and if the variance has to be minimised, then the model complexity has to be decreased. Both cannot be done simultaneously. This is the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q24. Explain different trade-offs in Machine Learning algorithms?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various types of Trade-offs in ML are :- \n",
    "- Bias-Variance Trade-off : states that average squared error is composed of $Bias^2$, $Variance$ and Irreducible error.  As Irreducible error cannot be reduced further, to reduce error further we can either reduce bias or variance. If bias has to be reduced, then the model complexity has to be increased, and if the variance has to be minimised, then the model complexity has to be decreased. Both cannot be done simultaneously.\n",
    "\n",
    "- Precision-Recall Trade-off : states that in classification tasks, precision defined as $\\frac{TP}{TP + FP}$ and recall defined as $\\frac{TP}{TP + FN}$, cannot be increased simultaneously. If one is increased, the other gets reduced.\n",
    "    \n",
    "- Accuracy-Interpretability Trade-off : states that in order to have a model with higher accuracy, the model needs to be complex enough to predict correctly, but beacuse of the complexity in the model architecture, the interpretability of the model suffers severely. And, for the model to be explainable, the model needs to be simpler, and hence, the model won't be very accurate with its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q25. What is cross-validation and how it is useful in training ML models?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Cross Validation, we split the dataset into train, test and validation set. We use the train set to train our model and use the validation set to test how the model is performing, also, check if the model is overfitting the train set.\n",
    "\n",
    "The Cross Validation technique that is used, most often, is the K-fold Cross-Validation. In this, we split the dataset into K-folds (subsets) and K-1 subsets are used for training and only one subset is used for validation. This step is repeated K times, such that all of the data get used for training as well as validation purposes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
